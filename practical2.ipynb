{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Practical 2:  Classifying TED Talks\n",
    "<p>A solution to the task for the Deep Learning for NLP 2017 course.<br>\n",
    "https://www.cs.ox.ac.uk/teaching/courses/2016-2017/dl/</p>\n",
    "<p>Tasks created by [Yannis Assael, Brendan Shillingford, Chris Dyer]</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from random import shuffle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"0ef2773d-2cc7-49f5-b272-d2d53a784ebe\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "    window._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (window._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    window._bokeh_timeout = Date.now() + 5000;\n",
       "    window._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (window.Bokeh !== undefined) {\n",
       "      document.getElementById(\"0ef2773d-2cc7-49f5-b272-d2d53a784ebe\").textContent = \"BokehJS successfully loaded.\";\n",
       "    } else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"0ef2773d-2cc7-49f5-b272-d2d53a784ebe\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '0ef2773d-2cc7-49f5-b272-d2d53a784ebe' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      document.getElementById(\"0ef2773d-2cc7-49f5-b272-d2d53a784ebe\").textContent = \"BokehJS is loading...\";\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((window.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!window._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      window._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"0ef2773d-2cc7-49f5-b272-d2d53a784ebe\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Part 0: Downloading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import lxml.etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Download the dataset if it's not already there: this may take a minute as it is 75MB\n",
    "if not os.path.isfile('ted_en-20160408.zip'):\n",
    "    urllib.request.urlretrieve(\"https://wit3.fbk.eu/get.php?path=XML_releases/xml/ted_en-20160408.zip&filename=ted_en-20160408.zip\", filename=\"ted_en-20160408.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# For now, we're only interested in the subtitle text, so let's extract that from the XML:\n",
    "with zipfile.ZipFile('ted_en-20160408.zip', 'r') as z:\n",
    "    doc = lxml.etree.parse(z.open('ted_en-20160408.xml', 'r'))\n",
    "all_text = '\\n'.join(doc.xpath('//content/text()'))\n",
    "talkcontent = doc.xpath('//content/text()')\n",
    "keywords = doc.xpath('//head/keywords/text()')\n",
    "talks = [(a,b) for (a,b) in zip(talkcontent, keywords)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The XML is traversed, and for each talk we obtain the text of the talk, as well as associated keywords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Thousands of years from now, we'll look back at the first century of computing as a fascinating but very peculiar time -- the only time in history where humans were reduced to live in 2D space, interacting with technology as if we were machines; a singular, 100-year period in the vastness of time where humans communicated, were entertained and managed their lives from behind a screen.\n",
      "Today, we spend most of our time tapping and looking at screens. What happened to interacting with each other? I ... ]\n",
      "Tagged as: talks, NASA, communication, computers, creativity, design, engineering, exploration, future, innovation, interface design, invention, microsoft, potential, prediction, product design, technology, visualizations\n",
      "2085 talks parsed.\n"
     ]
    }
   ],
   "source": [
    "talk, tag = talks[4]\n",
    "print ( \"[ \" + talk[:500] + \" ... ]\" )\n",
    "print( \"Tagged as: \" + tag)\n",
    "\n",
    "print(len(talks) , \"talks parsed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This then needs to be reformatted, so that only the relevant keywords are included, and the text content is tokenised in the same manner as the first practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def to_label(keywords):\n",
    "    labels = [x.strip() for x in keywords.split(',')]\n",
    "    \n",
    "    label1 = \"o\"\n",
    "    label2 = \"o\"\n",
    "    label3 = \"o\"\n",
    "    \n",
    "    if \"technology\" in labels:\n",
    "        label1 = \"T\"\n",
    "    if \"entertainment\" in labels:\n",
    "        label2 = \"E\"\n",
    "    if \"design\" in labels:\n",
    "        label3 = \"D\"\n",
    "        \n",
    "    return label1 + label2 + label3\n",
    "\n",
    "def to_one_hot(label):\n",
    "    if label == \"ooo\":\n",
    "        val = [1,0,0,0,0,0,0,0]\n",
    "    elif label == \"Too\":\n",
    "        val = [0,1,0,0,0,0,0,0]\n",
    "    elif label == \"oEo\":\n",
    "        val = [0,0,1,0,0,0,0,0]\n",
    "    elif label == \"ooD\":\n",
    "        val = [0,0,0,1,0,0,0,0]\n",
    "    elif label == \"TEo\":\n",
    "        val = [0,0,0,0,1,0,0,0]\n",
    "    elif label == \"oED\":\n",
    "        val = [0,0,0,0,0,1,0,0]\n",
    "    elif label == \"ToD\":\n",
    "        val = [0,0,0,0,0,0,1,0]\n",
    "    elif label == \"TED\":\n",
    "        val = [0,0,0,0,0,0,0,1]\n",
    "    else:\n",
    "        print(\"One hot invalid input: \", label)\n",
    "        val = [0,0,0,0,0,0,0,0]\n",
    "        \n",
    "    return np.asarray(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "processed_talks = list()\n",
    "labels = list()\n",
    "\n",
    "for (talk, keywords) in talks:\n",
    "    input_text_noparens = re.sub(r'\\([^)]*\\)', '', talk)\n",
    "\n",
    "    sentences_strings_ted = []\n",
    "\n",
    "    for line in input_text_noparens.split('\\n'):\n",
    "        m = re.match(r'^(?:(?P<precolon>[^:]{,20}):)?(?P<postcolon>.*)$', line)\n",
    "        sentences_strings_ted.extend(sent for sent in m.groupdict()['postcolon'].split('.') if sent)\n",
    "\n",
    "    sentences_ted = []\n",
    "        \n",
    "    for sent_str in sentences_strings_ted:\n",
    "        tokens = re.sub(r\"[^a-z0-9]+\", \" \", sent_str.lower()).split()\n",
    "        sentences_ted.extend(tokens)\n",
    "    \n",
    "    #There are a couple of talks without any actual parsable content - i.e. id 93. We will skip these\n",
    "    if(len(sentences_ted) == 0):\n",
    "        continue\n",
    "        \n",
    "    #Process keywords    \n",
    "    label = to_label(keywords)\n",
    "    \n",
    "    processed_talks.append(sentences_ted)\n",
    "    labels.append(label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now all talks are given as a list of tokens, with their keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thousands', 'of', 'years', 'from', 'now', 'we', 'll', 'look', 'back', 'at', 'the', 'first', 'century', 'of', 'computing', 'as', 'a', 'fascinating', 'but', 'very', 'peculiar', 'time', 'the', 'only', 'time', 'in', 'history', 'where', 'humans', 'were', 'reduced', 'to', 'live', 'in', '2d', 'space', 'interacting', 'with', 'technology', 'as', 'if', 'we', 'were', 'machines', 'a', 'singular', '100', 'year', 'period', 'in']\n",
      "ToD\n",
      "[0 0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "words = processed_talks[4]\n",
    "label = labels[4]\n",
    "print(words[0:50])\n",
    "print(label)\n",
    "print(to_one_hot(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooD\n",
      "ToD\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ToD\n",
      "ooo\n",
      "ooD\n",
      "oEo\n",
      "ToD\n",
      "ooo\n",
      "ooo\n",
      "ooD\n",
      "ooo\n",
      "ToD\n",
      "ooo\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,20):\n",
    "    print (labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Part 2: Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Randomly permute the dataset, and keep the last two blocks of 250 for validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1578 training items\n",
      "250 testing items\n",
      "250 validation items\n"
     ]
    }
   ],
   "source": [
    "zipped = list(zip(processed_talks, labels))\n",
    "\n",
    "shuffle(zipped)\n",
    "\n",
    "shuffled_talks, shuffeled_labels = zip(*zipped)\n",
    "\n",
    "data_training = shuffled_talks[:-500]\n",
    "labels_training = shuffeled_labels[:-500]\n",
    "print( str(len(data_training)) + \" training items\")  \n",
    "\n",
    "data_testing = shuffled_talks[-500:-250]\n",
    "labels_testing = shuffeled_labels[-500:-250]\n",
    "print( str(len(data_testing)) + \" testing items\") \n",
    "\n",
    "data_validation = shuffled_talks[-250:]\n",
    "labels_validation = shuffeled_labels[-250:]\n",
    "print( str(len(data_validation)) + \" validation items\")\n",
    "\n",
    "test_size = len(data_training)\n",
    "validation_size = len(data_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "1. Compare the learning curves of the model starting from random embeddings, starting from GloVe embeddings (http://nlp.stanford.edu/data/glove.6B.zip; 50 dimensions) or fixed to be the GloVe values. Training in batches is more stable (e.g. 50), which model works best on training vs. test? Which model works best on held-out accuracy?\n",
    "2. What happens if you try alternative non-linearities (logistic sigmoid or ReLU instead of tanh)?\n",
    "3. What happens if you add dropout to the network?\n",
    "4. What happens if you vary the size of the hidden layer?\n",
    "5. How would the code change if you wanted to add a second hidden layer?\n",
    "6. How does the training algorithm affect the quality of the model?\n",
    "7. Project the embeddings of the labels onto 2 dimensions and visualise (each row of the projection matrix V corresponds a label embedding). Do you see anything interesting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Download the glove embeddings\n",
    "if not os.path.isfile('glove.zip'):\n",
    "    urllib.request.urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", filename=\"glove.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\deeplearning\\\\glove.6B.50d.txt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = zipfile.ZipFile('glove.zip', 'r') \n",
    "glove_lines = [ line.decode(\"utf-8\").strip() for line in z.open('glove.6B.50d.txt', 'r').readlines() ] \n",
    "z.extract('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n"
     ]
    }
   ],
   "source": [
    "print(glove_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "embedding_size = 50\n",
    "vocabulary_size = len(glove_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "glove_embedding = { }\n",
    "\n",
    "for line in glove_lines:\n",
    "    words = line.split()\n",
    "    word = words[0]\n",
    "    embedding = [float(x) for x in words[1:] ]#map(float, words[1:])\n",
    "    glove_embedding[word] = embedding\n",
    "    \n",
    "glove_embedding['~'] = [0.0] * embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove embedding: \n",
      "the embedding: [ 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581]\n",
      "Vocabulary Size:  400000\n"
     ]
    }
   ],
   "source": [
    "def print_embedding(word, embedding) :\n",
    "    print (word + \" embedding: \" \"[ \" + \" \".join(map(str, embedding)) + \"]\")\n",
    "\n",
    "print( \"Glove embedding: \")\n",
    "print_embedding(\"the\", glove_embedding[\"the\"])\n",
    "\n",
    "print(\"Vocabulary Size: \", vocabulary_size)\n",
    "\n",
    "glove_embedding_mat = np.asarray(glove_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We need to map words which arent in our embedding to a special token, which we may define as '~'\n",
    "\n",
    "We also need a method to prepare the word embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_token(word, embedding_vec) :\n",
    "    if (word in embedding_vec):\n",
    "        return word\n",
    "    else:\n",
    "        return '~'\n",
    "    \n",
    "#Use bag of words structure - average over all word emebddings\n",
    "def get_embedding(text, embedding):\n",
    "        \n",
    "    if (len(text) == 0):\n",
    "        print(\"oop\")\n",
    "        \n",
    "    mean = np.array(np.zeros(embedding_size))\n",
    "    length = 0\n",
    "    \n",
    "    for word in text:\n",
    "        length += 1   \n",
    "        mean += embedding[get_token(word, embedding)]\n",
    "                   \n",
    "    mean /= length\n",
    "    \n",
    "    return np.asarray(mean)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: here....\n",
      "Embedding for text: [ 0.20422     0.69111     1.0191175   0.9112025  -0.1018875  -0.2443\n",
      "  0.30575    -0.762815    0.01082     0.0658945  -0.607105    0.464846\n",
      " -0.4133975  -0.73253    -0.2932875  -0.585892   -0.5807895  -0.3730975\n",
      "  0.0764475  -0.38028    -0.249845   -0.44856     0.9568675   0.8807275\n",
      " -0.2546175  -0.69559325 -0.23883875 -0.34764425 -0.06621    -0.434351\n",
      "  2.355375   -0.29038925 -0.1535075   0.17053    -0.075032   -0.99037\n",
      "  0.344277   -0.4631875   0.308017    0.26623825  1.37863     0.2944975\n",
      " -0.2656045   0.0956935   0.0872225   0.371975    0.21221125 -0.366875\n",
      "  0.2746625   1.48315   ]\n",
      "[  2.45736112e-01   1.76350005e-01  -1.08727932e-01  -5.91838211e-02\n",
      "   4.02004941e-01   1.77456605e-01  -3.28643840e-01  -7.70054963e-02\n",
      "  -1.57054298e-01   1.93708288e-02  -6.22838878e-02   1.23384084e-01\n",
      "  -3.61912225e-01  -1.04494202e-01   4.28112530e-01   1.83770147e-01\n",
      "   1.75551887e-01  -4.50511546e-02  -2.12766431e-01  -4.42405112e-01\n",
      "  -4.30177004e-02   2.14213367e-01   1.57131004e-01   7.31506569e-03\n",
      "   2.28627431e-01  -1.62274633e+00  -4.26166549e-01   2.64148869e-03\n",
      "   1.97393625e-01  -2.94202082e-01   3.15625786e+00   1.91140067e-01\n",
      "  -3.23147798e-01  -3.55048271e-01   2.56941215e-02  -1.28220694e-01\n",
      "   5.26086200e-02   1.20458857e-01   1.18189289e-02  -1.70820796e-01\n",
      "  -1.77557101e-01   6.06219992e-02   1.19585562e-03   2.96262480e-01\n",
      "  -8.81186659e-02   1.15728320e-01  -1.43333064e-01   1.81963012e-02\n",
      "  -6.54420282e-02   3.66003060e-02]\n"
     ]
    }
   ],
   "source": [
    "#https://www.tensorflow.org/tutorials/word2vec \n",
    "\n",
    "#We need to find the embedding size, this can be either retrieved from the glove dataset or by taking our embedding size as\n",
    "#the size of our largest sentence\n",
    "\n",
    "#practical 1 learnt embeddings by using the Word2Vec(sentences, size=100, min_count=10)\n",
    "#Not sure if this should still be used\n",
    "\n",
    "embedding = get_embedding(processed_talks[0][0], glove_embedding)\n",
    "\n",
    "print(\"Words: \" + str(processed_talks[0][0][:10]) + \"....\")\n",
    "print(\"Embedding for text: \" + str(embedding))\n",
    "\n",
    "embed_training = [get_embedding(words, glove_embedding) for words in data_training]\n",
    "embed_testing = [get_embedding(words, glove_embedding) for words in data_testing]\n",
    "embed_validation = [get_embedding(words, glove_embedding) for words in data_validation]\n",
    "\n",
    "print(embed_training[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "num_classes = 8\n",
    "hidden_size = 32\n",
    "\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Network Structure\n",
    "x = embedding(text)\n",
    "\n",
    "h = tanh(Wx + b)\n",
    "\n",
    "u = Vh + c\n",
    "\n",
    "p = softmax(u)\n",
    "\n",
    "prediction = argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X :  Tensor(\"Placeholder:0\", shape=(?, 50), dtype=float32)\n",
      "W :  Tensor(\"Variable/read:0\", shape=(50, 32), dtype=float32)\n",
      "h :  Tensor(\"Tanh:0\", shape=(?, 32), dtype=float32)\n",
      "V :  Tensor(\"Variable_1/read:0\", shape=(32, 8), dtype=float32)\n",
      "u :  Tensor(\"Add_1:0\", shape=(?, 8), dtype=float32)\n",
      "softmax :  Tensor(\"Softmax:0\", shape=(?, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Function to return (embedding, loss func, optimizer , ... )\n",
    "tf.reset_default_graph()\n",
    "#1.random embeddings\n",
    "#2. start from GloVe\n",
    "#3. fixed as GloVe\n",
    "\n",
    "#train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "#train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "#embedding_placeholder = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "#x = tf.placeholder(\"float\", shape=[None, embedding_size])\n",
    "\n",
    "\n",
    "x = tf.placeholder(\"float\", shape=[None, embedding_size])\n",
    "y = tf.placeholder(\"float\", shape=[None, num_classes])\n",
    "\n",
    "#embed = tf.nn.embedding_lookup(embedding_placeholder, x)\n",
    "\n",
    "W_weights = tf.Variable(\n",
    "  tf.truncated_normal([embedding_size, hidden_size],\n",
    "                      stddev=1.0 / math.sqrt(embedding_size)))\n",
    "\n",
    "V_weights = tf.Variable(\n",
    "  tf.truncated_normal([hidden_size, num_classes],\n",
    "                      stddev=1.0 / math.sqrt(embedding_size)))\n",
    "\n",
    "b = tf.Variable(tf.zeros([hidden_size]))\n",
    "c = tf.Variable(tf.zeros([num_classes]))\n",
    "\n",
    "# h = tanh(Wx + b)\n",
    "h = tf.tanh(tf.add(\n",
    "        tf.matmul(x, W_weights), b))\n",
    "            \n",
    "#Here, we can add a number of more hidden layers, i.e. a h' between h and u, which is built\n",
    "#similarily to h - activation ( hidden_weights * prev_layer + hidden_biases )\n",
    "\n",
    "#u = Vh + c\n",
    "u = tf.add(tf.matmul(h, V_weights), c)\n",
    "            \n",
    "p = tf.nn.softmax(u)\n",
    "print(\"X : \", x)\n",
    "print(\"W : \", W_weights)\n",
    "print(\"h : \", h)\n",
    "print(\"V : \", V_weights)\n",
    "print(\"u : \", u)\n",
    "print(\"softmax : \", p)\n",
    "#print(\"argmax: \", predict)\n",
    "\n",
    "#loss = tf.reduce_mean(\n",
    "#  tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels,\n",
    "#                 num_sampled, vocabulary_size))\n",
    "#loss = tf.nn.softmax_cross_entropy_with_logits(predict, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Adjust non-linearirities\n",
    "\n",
    "#Adjust dropout\n",
    "\n",
    "#Adjust hidden layer size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#here we create the optimizers, cost, prediction models\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=u, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "predict = tf.equal(tf.argmax(u,axis=0), tf.argmax(y,axis=0))\n",
    "accuracy = tf.reduce_mean(tf.cast(predict, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(graph, embedding, embedding_name, optimizer, loss, dropout = False, embedding_trainable = True):\n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        session.run( tf.global_variables_initializer())\n",
    "        print(\"Initialised training.\")\n",
    "        \n",
    "        #_embedding = tf.get_variable(name=embedding_name, shape=embedding.shape, \n",
    "        #        initializer=tf.constant_initializer(embedding), trainable=embedding_trainable)\n",
    "        \n",
    "        num_batches = (int) (test_size / batch_size)\n",
    "        print(\"Runing: \", num_batches, \" batches of size \", batch_size)\n",
    "        \n",
    "        for batch_index in range(0, num_batches):\n",
    "            start_index = batch_size * batch_index\n",
    "            end_index = (batch_index + 1) * batch_size \n",
    "\n",
    "            text_train = embed_training[start_index : end_index]\n",
    "            labels_train = [ to_one_hot(label) for label in labels_training[start_index : end_index]]\n",
    "            \n",
    "            text_test = embed_testing[0 : test_size]\n",
    "            labels_test = [ to_one_hot(label) for label in labels_testing[0 : test_size]]\n",
    "\n",
    "            #session.run([optimizer, loss], \n",
    "            #    feed_dict={x: text_train[0], y: labels_train[0]})\n",
    "            \n",
    "            #print(text_train[0])\n",
    "            #print(labels_train[0])\n",
    "            \n",
    "            optimizer.run(feed_dict={x: text_test, y: labels_test})  \n",
    "\n",
    "            opt, acc, cst = session.run([optimizer,accuracy,cost], \n",
    "                                        feed_dict={x: text_train, y: labels_train})\n",
    "            test_accuracy  = session.run(accuracy, \n",
    "                                        feed_dict={x: text_test, y: labels_test})\n",
    "            \n",
    "            \n",
    "            print(acc)\n",
    "            print(test_accuracy)\n",
    "            \n",
    "            print(\"train accuracy at epoch %d = %.2f%%, test accuracy = %.2f%%\"\n",
    "              % (batch_index, 100. * acc, 100. * test_accuracy))\n",
    "            \n",
    "        session.close()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Investigating training random embeddings on the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#glove_embeddings_fixed = tf.Variable(\n",
    " #   tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0), trainable = False)\n",
    "\n",
    "#glove_embeddings_trainable = tf.Variable(\n",
    "  #  tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0), trainable = True)\n",
    "\n",
    "#embed_random = tf.nn.embedding_lookup(random_embedding, train_inputs)\n",
    "\n",
    "#loss_random = tf.reduce_mean(\n",
    "#      tf.nn.nce_loss(nce_weights, nce_biases, embed_random, train_labels,\n",
    "#                 num_sampled, vocabulary_size))\n",
    "\n",
    "#loss_logits = tf.reduce_mean(\n",
    "#    tf.nn.softmax_cross_entropy_with_logits(\n",
    "#        labels=train_labels, logits=yhat ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised training.\n",
      "Runing:  31  batches of size  50\n",
      "0.0\n",
      "0.0\n",
      "train accuracy at epoch 0 = 0.00%, test accuracy = 0.00%\n",
      "0.0\n",
      "0.0\n",
      "train accuracy at epoch 1 = 0.00%, test accuracy = 0.00%\n",
      "0.0\n",
      "0.0\n",
      "train accuracy at epoch 2 = 0.00%, test accuracy = 0.00%\n",
      "0.0\n",
      "0.0\n",
      "train accuracy at epoch 3 = 0.00%, test accuracy = 0.00%\n",
      "0.0\n",
      "0.0\n",
      "train accuracy at epoch 4 = 0.00%, test accuracy = 0.00%\n",
      "0.0\n",
      "0.0\n",
      "train accuracy at epoch 5 = 0.00%, test accuracy = 0.00%\n",
      "0.0\n",
      "0.0\n",
      "train accuracy at epoch 6 = 0.00%, test accuracy = 0.00%\n",
      "0.125\n",
      "0.0\n",
      "train accuracy at epoch 7 = 12.50%, test accuracy = 0.00%\n",
      "0.125\n",
      "0.0\n",
      "train accuracy at epoch 8 = 12.50%, test accuracy = 0.00%\n",
      "0.0\n",
      "0.0\n",
      "train accuracy at epoch 9 = 0.00%, test accuracy = 0.00%\n",
      "0.0\n",
      "0.0\n",
      "train accuracy at epoch 10 = 0.00%, test accuracy = 0.00%\n",
      "0.0\n",
      "0.0\n",
      "train accuracy at epoch 11 = 0.00%, test accuracy = 0.00%\n",
      "0.0\n",
      "0.0\n",
      "train accuracy at epoch 12 = 0.00%, test accuracy = 0.00%\n",
      "0.125\n",
      "0.0\n",
      "train accuracy at epoch 13 = 12.50%, test accuracy = 0.00%\n",
      "0.125\n",
      "0.0\n",
      "train accuracy at epoch 14 = 12.50%, test accuracy = 0.00%\n",
      "0.0\n",
      "0.0\n",
      "train accuracy at epoch 15 = 0.00%, test accuracy = 0.00%\n",
      "0.0\n",
      "0.0\n",
      "train accuracy at epoch 16 = 0.00%, test accuracy = 0.00%\n",
      "0.125\n",
      "0.0\n",
      "train accuracy at epoch 17 = 12.50%, test accuracy = 0.00%\n",
      "0.125\n",
      "0.0\n",
      "train accuracy at epoch 18 = 12.50%, test accuracy = 0.00%\n",
      "0.0\n",
      "0.0\n",
      "train accuracy at epoch 19 = 0.00%, test accuracy = 0.00%\n",
      "0.125\n",
      "0.0\n",
      "train accuracy at epoch 20 = 12.50%, test accuracy = 0.00%\n",
      "0.25\n",
      "0.0\n",
      "train accuracy at epoch 21 = 25.00%, test accuracy = 0.00%\n",
      "0.0\n",
      "0.0\n",
      "train accuracy at epoch 22 = 0.00%, test accuracy = 0.00%\n",
      "0.0\n",
      "0.0\n",
      "train accuracy at epoch 23 = 0.00%, test accuracy = 0.00%\n",
      "0.125\n",
      "0.0\n",
      "train accuracy at epoch 24 = 12.50%, test accuracy = 0.00%\n",
      "0.0\n",
      "0.0\n",
      "train accuracy at epoch 25 = 0.00%, test accuracy = 0.00%\n",
      "0.0\n",
      "0.0\n",
      "train accuracy at epoch 26 = 0.00%, test accuracy = 0.00%\n",
      "0.0\n",
      "0.0\n",
      "train accuracy at epoch 27 = 0.00%, test accuracy = 0.00%\n",
      "0.25\n",
      "0.0\n",
      "train accuracy at epoch 28 = 25.00%, test accuracy = 0.00%\n",
      "0.375\n",
      "0.0\n",
      "train accuracy at epoch 29 = 37.50%, test accuracy = 0.00%\n",
      "0.0\n",
      "0.0\n",
      "train accuracy at epoch 30 = 0.00%, test accuracy = 0.00%\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "random_embedding = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0), trainable = True)\n",
    "\n",
    "train(graph, random_embedding, 'random_embedding', optimizer, cost, embedding_trainable = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#tSNE visualisation\n",
    "\n",
    "#Get each row of V, project onto tSNE"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
